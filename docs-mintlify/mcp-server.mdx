---
title: 'MCP Server'
description: 'Enable AI assistants to analyze performance traces and generate flamegraph visualizations with the Flamechart MCP server.'
---

The Flamechart MCP (Model Context Protocol) server enables AI assistants to analyze performance traces and generate flamegraph visualizations. It works with both local trace files and remote traces stored on FlameDeck.

<Info>
**Works Entirely Offline** - No API key required for local trace files! Only need a FlameDeck API key for analyzing remote traces.
</Info>

## Quick Start

### 1. Installation & Setup

The MCP server can be run directly without installation:

```bash
npx -y @flamedeck/flamechart-mcp
```

### 2. Configure Your AI Assistant

<Tabs>
  <Tab title="Cursor">
    Add to your Cursor MCP configuration:

    ```json
    {
      "mcpServers": {
        "flamechart-debug": {
          "command": "npx",
          "args": ["-y", "@flamedeck/flamechart-mcp"]
        }
      }
    }
    ```
  </Tab>
  <Tab title="Claude Desktop">
    Add to `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS):

    ```json
    {
      "mcpServers": {
        "flamechart-debug": {
          "command": "npx",
          "args": ["-y", "@flamedeck/flamechart-mcp"]
        }
      }
    }
    ```
  </Tab>
</Tabs>

### 3. Start Analyzing

Try this prompt in your AI assistant:

```
Analyze this trace file and show me the top 10 slowest functions:
/path/to/your/profile.json
```

## Supported Trace Formats

The MCP server supports a wide range of profiling formats, including pprof, chrome, pyinstrument, etc.

<Check>
**Automatic Format Detection** - The server automatically detects trace formats and handles gzipped files transparently.
</Check>

## Available Tools

### get_top_functions

Identifies the slowest functions in your trace, helping you find performance bottlenecks.

<ParamField path="trace" type="string" required>
  File path or FlameDeck URL
</ParamField>

<ParamField path="sortBy" type="string" default="total">
  Sort by `'self'` or `'total'` time
</ParamField>

<ParamField path="offset" type="number" default="0">
  Starting position for pagination
</ParamField>

<ParamField path="limit" type="number" default="15">
  Number of functions to return
</ParamField>

**Example Usage:**
```
Show me the top 20 functions consuming the most total time in this trace:
/Users/dev/profiles/api-slow.cpuprofile
```

### generate_flamegraph_screenshot

Creates a visual flamegraph showing the timeline of function execution.

<ParamField path="trace" type="string" required>
  File path or FlameDeck URL
</ParamField>

<ParamField path="width" type="number" default="1200">
  Image width in pixels
</ParamField>

<ParamField path="height" type="number" default="800">
  Image height in pixels
</ParamField>

<ParamField path="mode" type="string" default="light">
  Theme: `'light'` or `'dark'`
</ParamField>

<ParamField path="startTimeMs" type="number">
  Start time for zoomed view
</ParamField>

<ParamField path="endTimeMs" type="number">
  End time for zoomed view
</ParamField>

<ParamField path="startDepth" type="number">
  Stack depth to start visualization
</ParamField>

**Example Usage:**
```
Generate a dark mode flamegraph of this trace for my presentation:
/path/to/production-profile.json
```

### generate_sandwich_flamegraph_screenshot

Creates a "sandwich view" focusing on a specific function, showing both its callers and callees.

<ParamField path="trace" type="string" required>
  File path or FlameDeck URL
</ParamField>

<ParamField path="frameName" type="string" required>
  Exact function name to focus on
</ParamField>

**Example Usage:**
```
Create a sandwich view for the "processData" function to understand what's calling it:
/path/to/trace.json
```

## Working with Remote Traces

### Setting Up FlameDeck Integration

<Steps>
  <Step title="Create an API Key">
    - Go to [FlameDeck Settings](https://flamedeck.com/settings/api-keys)
    - Create a new key with `trace:download` permissions
    - Copy the generated key
  </Step>

  <Step title="Configure the MCP Server">
    For environment variable approach:
    ```bash
    export FLAMEDECK_API_KEY="your_api_key_here"
    npx -y @flamedeck/flamechart-mcp
    ```

    For direct configuration:
    ```json
    {
      "mcpServers": {
        "flamechart-debug": {
          "command": "npx",
          "args": ["-y", "@flamedeck/flamechart-mcp"],
          "env": {
            "FLAMEDECK_API_KEY": "your_api_key_here"
          }
        }
      }
    }
    ```
  </Step>
</Steps>

### Using Remote Traces

Once configured, you can analyze traces directly from FlameDeck URLs:

```
Analyze this production trace and find performance bottlenecks:
https://www.flamedeck.com/traces/98508d02-1f2a-4885-9607-ecadceb3d734

Focus on database operations and API endpoints.
```

## Performance Features

### Smart Caching

The MCP server includes intelligent caching to improve performance:

- **Remote URLs**: Cached for 5 minutes to avoid repeated API calls
- **Local Files**: Cached until file modification detected
- **Memory Management**: Automatic cleanup with 50 profile limit
- **Cache Invalidation**: Smart invalidation based on file changes

<Tip>
**Performance Boost** - Subsequent tool calls on the same trace are nearly instant thanks to smart caching.
</Tip>

## Example Workflows

### Debugging Slow React Rendering

```
I have a React app that's rendering slowly. Analyze this Chrome DevTools profile:
/Users/dev/profiles/react-slow-render.json

Please:
1. Show me the top functions taking the most time
2. Generate a flamegraph to visualize the call stack
3. Look for any React-specific bottlenecks like expensive re-renders
4. Focus on functions in my application code vs React internals
```

### API Performance Investigation

```
Our API endpoints are slow. Help me analyze this production trace:
https://www.flamedeck.com/traces/api-performance-issue

Investigation goals:
1. Identify the slowest endpoints
2. Find database vs application time breakdown
3. Look for N+1 query patterns
4. Generate a visual report I can share with the team
```

### Memory Leak Detection

```
Investigate potential memory leaks in this Node.js heap profile:
/path/to/heap-snapshot.heapprofile

Focus on:
1. Functions with high memory allocation
2. Objects that aren't being garbage collected
3. Create a sandwich view for suspicious allocation patterns
```

## Troubleshooting

### Common Issues

<AccordionGroup>
  <Accordion title="MCP Server Not Starting">
    - Ensure Node.js is installed (version 16+ recommended)
    - Try running with `npx -y @flamedeck/flamechart-mcp` to force latest version
  </Accordion>

  <Accordion title="API Key Authentication Errors">
    - Verify your API key has `trace:download` permissions
    - Check that the key is correctly set in environment variables
    - Ensure the FlameDeck URL format is correct: `https://www.flamedeck.com/traces/{id}`
  </Accordion>

  <Accordion title="Trace File Not Found">
    - Use absolute file paths for local traces
    - Verify file exists and is readable
    - Check that the trace format is supported
  </Accordion>

  <Accordion title="Performance Issues">
    - Large traces (>100MB) may take longer to process
    - Consider using time-based zooming for very long traces
    - Clear cache if memory usage becomes excessive
  </Accordion>
</AccordionGroup>

### Getting Help

If you encounter issues:

1. **Check the Console**: The MCP server logs detailed information about loading and caching
2. **Verify File Format**: Ensure your trace is in a supported format
3. **Test with Sample Data**: Try with a known-good trace file first
4. **API Key Permissions**: Confirm your FlameDeck API key has the correct scope

<Warning>
**Large File Performance** - For traces larger than 100MB, consider using time-based filtering to focus on specific time ranges.
</Warning>

## Advanced Usage

### Custom Flamegraph Views

Create focused visualizations by specifying time ranges:

```
Generate a flamegraph focusing on the 500ms-1500ms time range of this trace:
/path/to/long-running-profile.json

I want to see what happened during that specific slow period.
```

### Integration with Code Analysis

Combine trace analysis with code review:

```
Analyze this performance trace and then look through my codebase to understand the bottlenecks:
/path/to/slow-api.cpuprofile

Focus on the slowest functions and help me understand:
1. What those functions actually do in the code
2. Why they might be slow
3. Potential optimization strategies
``` 